{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting word2number\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "Building wheels for collected packages: word2number\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5586 sha256=b24810801426cfe0a8d606c27abef836c8903b7813034d1713e8cc44716bd896\n",
      "  Stored in directory: /Users/yohanjhaveri/Library/Caches/pip/wheels/a0/4a/5b/d2f2df5c344ddbecb8bea759872c207ea91d93f57fb54e816e\n",
      "Successfully built word2number\n",
      "Installing collected packages: word2number\n",
      "Successfully installed word2number-1.1\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=4b6756213d0db1a20e8d75387cdfe3a5d48bc34bca41565bf5503f05cfb1a188\n",
      "  Stored in directory: /Users/yohanjhaveri/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.10\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install word2number\n",
    "!pip install num2words\n",
    "!pip install pyspellchecker\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from word2number import w2n\n",
    "from num2words import num2words\n",
    "from typing import List\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Processing\n",
    "# 1. Expand English contractions\n",
    "# 2. Convert written words to numbers\n",
    "# 3. Segment by sentences, then segment by clauses\n",
    "# 4. Spell checker\n",
    "# 5. Case folding, remove non-alphanumeric characters, remove stop-words\n",
    "# 6. Stemming and lemmatization\n",
    "# 7. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize methods\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # tokenize with . ! ? to segment sentences \n",
    "    RE_TOK = re.compile(r'([.!?]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_clauses(text):\n",
    "    # tokenize with , : ; to segment clauses \n",
    "    RE_TOK = re.compile(r'([,:;]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_regex(text):\n",
    "    # tokenize with . ! ? , : ; to segment sentences and clauses \n",
    "    RE_TOK = re.compile(r'([.,:]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand English Contractions \n",
    "# list of contractions from https://englishstudypage.com/grammar/list-of-contractions-in-english/\n",
    "\n",
    "# Read the English contractions list csv file and convert to dictionary where key = abbreviation, value = contracted words\n",
    "with open('english-contractions-list.csv', mode='r') as input:\n",
    "        reader = csv.reader(input)\n",
    "        contractionsDict = {rows[0]:rows[1] for rows in reader}\n",
    "        contractionsDict.pop('Abbreviation')\n",
    "    \n",
    "def expand_contractions(text):\n",
    "    result = text\n",
    "    tokens = tokenize_regex(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        # if the token is an abbreviation, then we will change it to its contracted English words\n",
    "        if contractionsDict.get(token.lower()) != None: \n",
    "            new = contractionsDict.get(token.lower())\n",
    "\n",
    "            # if the token was the beginning of a sentence, then we change the capitalization to match it\n",
    "            if token[:1].isupper(): new = new[:1].upper() + new[1:]   \n",
    "            result = result.replace(token, new)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert written words to numbers\n",
    "\n",
    "def word_to_number(text):\n",
    "    # regular expression to find written number words and digits\n",
    "    r = re.compile(r'(\\d)|\\b(zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|fourty|fifty|sixty|seventy|eight|ninety|hundred|thousand|million|billion|point)\\b')\n",
    "    r_digits = re.compile(r'(\\d)')\n",
    "    \n",
    "    result = text\n",
    "    tokens = tokenize_regex(text)\n",
    "    words = []\n",
    "    \n",
    "    fraction = False\n",
    "    mix = False\n",
    "    for index in range(len(tokens)):\n",
    "\n",
    "        # if we find a word that represents a number\n",
    "        if r.match(tokens[index].lower()): \n",
    "            words.append(tokens[index])\n",
    "        \n",
    "        # if we find a fraction\n",
    "        elif index+1 < len(tokens) and tokens[index].lower() == 'out' and tokens[index+1].lower() == 'of':\n",
    "            words.append(tokens[index])\n",
    "            words.append(tokens[index+1])\n",
    "            fraction = True\n",
    "            \n",
    "        # if we reach here, then tokens[index] is not a number word but our previous token(s) were number words\n",
    "        elif words:\n",
    "            old, replace = \"\", \"\"\n",
    "            for w in words: \n",
    "                # if there is a mix of digits and number words\n",
    "                if r_digits.match(w):\n",
    "                    replace = old + num2words(w) + \" \"\n",
    "                    mix = True \n",
    "                elif mix: replace = replace + w + \" \"\n",
    "                old = old + w + \" \"\n",
    "                    \n",
    "            old = old[:-1]\n",
    "            if replace: replace = replace[:-1]\n",
    "            \n",
    "            # if old contains \"out of\", then we add a slash for the fraction\n",
    "            if \"out of\" in old: \n",
    "                if r_digits.match(old): result = result.replace(\" out of\", \"/\")\n",
    "                else: result = result.replace(old, str(w2n.word_to_num(str(old))) + '/')\n",
    "                   \n",
    "            # if we didn't have this \"six out of ten\" would be converted to \"6/ 10\" instead of \"6/10\"\n",
    "            elif fraction: \n",
    "                result = result.replace(\" \" + old, str(w2n.word_to_num(old)))\n",
    "                fraction = False\n",
    "            \n",
    "            # if the text we are converting has a mix of words and digits (eg. \"fifty 4\")\n",
    "            elif mix: \n",
    "                result = result.replace(old, str(w2n.word_to_num(replace)))\n",
    "                mix = False\n",
    "                \n",
    "            # w2n.word_to_num() handles the conversion of the number words to number form\n",
    "            else: result = result.replace(old, str(w2n.word_to_num(old)))\n",
    "            words.clear()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence and Clause Segmentation\n",
    "\n",
    "r_digits = re.compile(r'\\d')\n",
    "\n",
    "def segment_sentence(text):\n",
    "    result = []\n",
    "    sentence = \"\"\n",
    "    tokens = tokenize_sentences(text)\n",
    "    \n",
    "    skip = False\n",
    "    for index in range(len(tokens)):   \n",
    "        \n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        \n",
    "        # beginning of a new sentence\n",
    "        elif sentence == \"\": sentence = tokens[index]\n",
    "            # sentence = tokens[index][:1].lower() + tokens[index][1:] #consider case folding\n",
    "        \n",
    "        # segment by sentences\n",
    "        elif tokens[index] in {'.','!','?'}:\n",
    "            \n",
    "            # unambiguous punctuation that signifies end of a sentence\n",
    "            if tokens[index] in {'!','?'}:\n",
    "                result.append(sentence)\n",
    "                sentence = \"\"\n",
    "            \n",
    "            # ambiguous punctuation that may or may not signify end of a sentece\n",
    "            elif tokens[index] == '.':\n",
    "                # period [.] represents a decimal number\n",
    "                if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                    sentence = sentence + tokens[index] + tokens[index+1] \n",
    "                    skip = True\n",
    "               \n",
    "                # period [.] represents common abbreviations like e.g. and i.e.\n",
    "                elif index+1 < len(tokens) and tokens[index-1] in {'e', 'i'} and tokens[index+1] in {'g','e'}: \n",
    "                    sentence = sentence + tokens[index+1]\n",
    "                    skip = True\n",
    "                \n",
    "                # period[.] represents the end of an abbreviation with periods inbetween\n",
    "                elif sentence[-2:] in {'eg','ie'}: continue\n",
    "                    \n",
    "                # period [.] represents end of common abbreviations with no periods inbetween\n",
    "                elif index-1 >= 0 and tokens[index-1].lower() in {'am','pm','mr','ms','dr','mrs','inc','tbsp','tsp','gal','lb','lbs','qt','pt'}: continue\n",
    "                \n",
    "                # period [.] represents the end of a sentence\n",
    "                else: \n",
    "                    result.append(sentence)\n",
    "                    sentence = \"\"\n",
    "            \n",
    "            # punctuation represents token is a clause (, ; :)\n",
    "            # else: sentence = sentence + tokens[index]\n",
    "        \n",
    "        else: sentence = sentence + \" \" + tokens[index]\n",
    "    if sentence: result.append(sentence)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def segment_clause(sentences: List[str]):\n",
    "    result = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        clause = \"\"\n",
    "        tokens = tokenize_clauses(sentence)\n",
    "        \n",
    "        skip = False\n",
    "        for index in range(len(tokens)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            \n",
    "            # beginning of a new clause\n",
    "            elif clause == \"\": \n",
    "                clause = tokens[index]\n",
    "            \n",
    "            elif tokens[index] in {',',';',':'}:\n",
    "                # unambiguous punctuation that signifies end of a clause\n",
    "                if tokens[index] in {';'}:\n",
    "                    result.append(clause)\n",
    "                    clause = \"\"\n",
    "                \n",
    "                # ambiguous colons [:] that may or may not signify the end of a clause\n",
    "                elif tokens[index] == ':':\n",
    "                    # colon [:] represents a ratio\n",
    "                    if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                        clause = clause + tokens[index] + tokens[index+1] \n",
    "                        skip = True\n",
    "                    \n",
    "                    # colon [:] represents end of a clause\n",
    "                    else: \n",
    "                        result.append(clause)\n",
    "                        clause = \"\"\n",
    "                \n",
    "                # ambiguous commas [,] that may or may not signify the end of a clause\n",
    "                else: \n",
    "                    # comma [,] represents a number with commas\n",
    "                    if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                        clause = clause + tokens[index] + tokens[index+1] \n",
    "                        skip = True\n",
    "                        \n",
    "                    # comma [,] used to separate items in a list\n",
    "                    elif index-2 >= 0 and tokens[index-2] == ',' or index+2 < len(tokens) and tokens[index+2] == ',': continue\n",
    "                    \n",
    "                    # comma [,] represents end of a clause\n",
    "                    else: \n",
    "                        result.append(clause)\n",
    "                        clause = \"\"\n",
    "            \n",
    "            else: clause = clause + \" \" + tokens[index]\n",
    "\n",
    "        if clause:\n",
    "            result.append(clause)\n",
    "            clause = \"\"\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Checker\n",
    "\n",
    "def spell_check(clauses: List[str]):\n",
    "    spell = SpellChecker()\n",
    "    result = clauses.copy()\n",
    "    \n",
    "    # for each clause in result\n",
    "    for index in range(len(result)):\n",
    "        \n",
    "        # split each clause by whitespaces\n",
    "        for word in result[index].split():\n",
    "            mispelled = spell.unknown(word)\n",
    "            if mispelled: result[index] = result[index].replace(word, spell.correction(word))\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mihir's added code:\n",
    "\n",
    "#  Case Folding\n",
    "def case_fold(clauses: List[str]):\n",
    "    result = clauses.copy()\n",
    "    for index in range(len(clauses)):\n",
    "        clauses[index] = clauses[index].lower()\n",
    "    return clauses\n",
    "\n",
    "# Removing non-alphanumeric characters\n",
    "def remove_non_alphanumeric(clauses: List[str]):\n",
    "    result = []\n",
    "    for clause in clauses:\n",
    "        clause = re.sub(r'[^a-zA-Z0-9./ ]', '', clause)\n",
    "        result.append(clause)\n",
    "    return result\n",
    "\n",
    "# Removing stop-words\n",
    "def remove_stop_words(clauses: List[str]):\n",
    "    #\"Long Stopword List\" from https://www.ranks.nl/stopwords\n",
    "\n",
    "    result = clauses.copy()\n",
    "    stop_words_string = \"a able about above abst accordance according accordingly across act actually added adj affected affecting affects after afterwards again against ah all almost alone along already also although always am among amongst an and announce another any anybody anyhow anymore anyone anything anyway anyways anywhere apparently approximately are aren arent arise around as aside ask asking at auth available away awfully b back be became because become becomes becoming been before beforehand begin beginning beginnings begins behind being believe below beside besides between beyond biol both brief briefly but by c ca came can cannot can't cause causes certain certainly co com come comes contain containing contains could couldnt d date did didn't different do does doesn't doing done don't down downwards due during e each ed edu effect eg eight eighty either else elsewhere end ending enough especially et et-al etc even ever every everybody everyone everything everywhere ex except f far few ff fifth first five fix followed following follows for former formerly forth found four from further furthermore g gave get gets getting give given gives giving go goes gone got gotten h had happens hardly has hasn't have haven't having he hed hence her here hereafter hereby herein heres hereupon hers herself hes hi hid him himself his hither home how howbeit however hundred i id ie if i'll im immediate immediately importance important in inc indeed index information instead into invention inward is isn't it itd it'll its itself i've j just k keep\tkeeps kept kg km know known knows l largely last lately later latter latterly least less lest let lets like liked likely line little 'll look looking looks ltd m made mainly make makes many may maybe me mean means meantime meanwhile merely mg might million miss ml more moreover most mostly mr mrs much mug must my myself n na name namely nay nd near nearly necessarily necessary need needs neither never nevertheless new next nine ninety no nobody non none nonetheless noone nor normally nos not noted nothing now nowhere o obtain obtained obviously of off often oh ok okay old omitted on once one ones only onto or ord other others otherwise ought our ours ourselves out outside over overall owing own p page pages part particular particularly past per perhaps placed please plus poorly possible possibly potentially pp predominantly present previously primarily probably promptly proud provides put q que quickly quite qv r ran rather rd re readily really recent recently ref refs regarding regardless regards related relatively research respectively resulted resulting results right run s said same saw say saying says sec section see seeing seem seemed seeming seems seen self selves sent seven several shall she shed she'll shes should shouldn't show showed shown showns shows significant significantly similar similarly since six slightly so some somebody somehow someone somethan something sometime sometimes somewhat somewhere soon sorry specifically specified specify specifying still stop strongly sub substantially successfully such sufficiently suggest sup sure\tt take taken taking tell tends th than thank thanks thanx that that'll thats that've the their theirs them themselves then thence there thereafter thereby thered therefore therein there'll thereof therere theres thereto thereupon there've these they theyd they'll theyre they've think this those thou though thoughh thousand throug through throughout thru thus til tip to together too took toward towards tried tries truly try trying ts twice two u un under unfortunately unless unlike unlikely until unto up upon ups us use used useful usefully usefulness uses using usually v value various 've very via viz vol vols vs w want wants was wasnt way we wed welcome we'll went were werent we've what whatever what'll whats when whence whenever where whereafter whereas whereby wherein wheres whereupon wherever whether which while whim whither who whod whoever whole who'll whom whomever whos whose why widely willing wish with within without wont words world would wouldnt www x y yes yet you youd you'll your youre yours yourself yourselves you've z zero\"\n",
    "    stop_words = stop_words_string.split()\n",
    "    \n",
    "\n",
    "    for index in range(len(result)):\n",
    "        filtered = []\n",
    "        \n",
    "        for word in result[index].split():\n",
    "            if word in stop_words: continue\n",
    "            filtered.append(word)\n",
    "        result[index] = \" \".join(filtered)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: I'd a turkey cheese sandwich with an apple and a glass of water\n",
      "\n",
      "expand_contractions: I would a turkey cheese sandwich with an apple and a glass of water\n",
      "\n",
      "word to numbers: I would a turkey cheese sandwich with an apple and a glass of water\n",
      "\n",
      "segment_sentence: ['I would a turkey cheese sandwich with an apple and a glass of water']\n",
      "\n",
      "segment_clause: ['I would a turkey cheese sandwich with an apple and a glass of water']\n",
      "\n",
      "spell_check: ['i would a turkey cheese sandwich with an apple and a glass of water']\n",
      "\n",
      "case_fold: ['i would a turkey cheese sandwich with an apple and a glass of water']\n",
      "\n",
      "remove_non_alphanumeric: ['i would a turkey cheese sandwich with an apple and a glass of water']\n",
      "\n",
      "remove_stop_words: ['turkey cheese sandwich apple glass water']\n"
     ]
    }
   ],
   "source": [
    "#text = \"Hello there, my name is: Chloe Lam! I'd like a cheese, tomato, parmesan, and egg sandwich. I had 3.5 eggs, e.g. on toast at 5 pm. today. There has been 1:2 ratios; I would like 3,500 of those\"\n",
    "# text = \"I beleeve I had one avocado toast, 2 point five slices of tomatoes, and 3 out of 4 pieces of chocolate. I'd rather have had an egg; they'd love that\"\n",
    "# text = \"I had 1 cup of basmati rice with chicken tikka masala\"\n",
    "text = \"I'd like a turkey cheese sandwich with an apple and a glass of water\"\n",
    "\n",
    "text_ec = expand_contractions(text)\n",
    "text_w2n = word_to_number(text_ec)\n",
    "text_ss = segment_sentence(text_w2n)\n",
    "text_sc = segment_clause(text_ss)\n",
    "text_sp = spell_check(text_sc)\n",
    "text_cf = case_fold(text_sp)\n",
    "text_rna = remove_non_alphanumeric(text_cf)\n",
    "text_rsw = remove_stop_words(text_rna)\n",
    "\n",
    "print(\"original:\", text)\n",
    "print()\n",
    "print(\"expand_contractions:\", text_ec)\n",
    "print()\n",
    "print(\"word to numbers:\", text_w2n)\n",
    "print()\n",
    "print(\"segment_sentence:\", text_ss)\n",
    "print()\n",
    "print(\"segment_clause:\", text_sc)\n",
    "print()\n",
    "print(\"spell_check:\", text_sp)\n",
    "print()\n",
    "print(\"case_fold:\", text_cf)\n",
    "print()\n",
    "print(\"remove_non_alphanumeric:\", text_rna)\n",
    "print()\n",
    "print(\"remove_stop_words:\", text_rsw)\n",
    "\n",
    "# Processing\n",
    "# 1. Expand English contractions\n",
    "# 2. Convert written words to numbers\n",
    "# 3. Segment by sentences, then segment by clauses\n",
    "# 4. Spell checker\n",
    "# 5. Case folding, remove non-alphanumeric characters, remove stop-words\n",
    "# 6. Stemming and lemmatization\n",
    "# 7. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
