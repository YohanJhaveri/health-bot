{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "import re\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # tokenize with . ! ? , : ; to segment sentences and clauses \n",
    "    RE_TOK = re.compile(r'([.!?]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_clauses(text):\n",
    "    # tokenize with . ! ? , : ; to segment sentences and clauses \n",
    "    RE_TOK = re.compile(r'([,:;]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_regex(text):\n",
    "    # tokenize with . ! ? , : ; to segment sentences and clauses \n",
    "    RE_TOK = re.compile(r'([.,:]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Checker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def spell_check(text):\n",
    "    spell = SpellChecker()\n",
    "    result = text\n",
    "    tokens = tokenize_regex(text)\n",
    "    \n",
    "    for index in range(len(tokens)):\n",
    "        mispelled = spell.unknown([tokens[index]])\n",
    "\n",
    "        if mispelled: \n",
    "            result = result.replace(tokens[index], spell.correction(tokens[index]))\n",
    "            tokens[index] = spell.correction(tokens[index])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in ./venv/lib/python3.9/site-packages (0.6.2)\r\n"
     ]
    }
   ],
   "source": [
    "# Expand English Contractions \n",
    "# list of contractions from https://englishstudypage.com/grammar/list-of-contractions-in-english/\n",
    "!pip install pyspellchecker\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read the English contractions list csv file and convert to dictionary where key = abbreviation, value = contracted words\n",
    "with open('english-contractions-list.csv', mode='r') as input:\n",
    "        reader = csv.reader(input)\n",
    "        contractionsDict = {rows[0]:rows[1] for rows in reader}\n",
    "        contractionsDict.pop('Abbreviation')\n",
    "    \n",
    "def expand_contractions(text):\n",
    "    result = text\n",
    "    tokens = tokenize_regex(text)\n",
    "    \n",
    "#     r = re.compile(r'\\b(aren\\'t|can\\'t|couldn\\'t|didn\\'t|don\\'t|doesn\\'t|hadn\\'t|haven\\'t)\\b'\n",
    "#                    r'\\b(he\\'s|he\\'ll|he\\'d|here\\'s|I\\'m|I\\'ve|I\\'ll|I\\'d|isn\\'t|it\\'s|it\\'ll)\\b'\n",
    "#                    r'\\b(mustn\\'t|she\\'s|she\\'ll|she\\'d|shouldn\\'t|that\\'s|there\\'s|they\\'re)\\b'\n",
    "#                    r'\\b(they\\'ve|they\\'ll|they\\'d|wasn\\'t|we\\'re|we\\'ve|we\\'ll|we\\'d|weren\\'t)\\b'\n",
    "#                    r'\\b(what\\'s|where\\'s|who\\'s|who\\'ll|won\\'t|wouldn\\'t|you\\'re|you\\'ve|you\\'ll)\\b'\n",
    "#                    r'\\b(you\\'d)\\b')\n",
    "    \n",
    "    for token in tokens:\n",
    "        # If the token is an abbreviation, then we will change it to its contracted English words\n",
    "        if contractionsDict.get(token.lower()) != None: \n",
    "            new = contractionsDict.get(token.lower())\n",
    "\n",
    "            # If the token was the beginning of a sentence, then we change the capitalization to match it\n",
    "            if token[:1].isupper(): new = new[:1].upper() + new[1:]   \n",
    "            result = result.replace(token, new)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence and Clause Segmentation\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "r_digits = re.compile(r'\\d')\n",
    "\n",
    "def segment_sentence(text):\n",
    "    result = []\n",
    "    sentence = \"\"\n",
    "    tokens = tokenize_sentences(text)\n",
    "    \n",
    "    skip = False\n",
    "    for index in range(len(tokens)):   \n",
    "        \n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        \n",
    "        # beginning of a new sentence\n",
    "        elif sentence == \"\": sentence = tokens[index]\n",
    "        \n",
    "        # segment by sentences\n",
    "        elif tokens[index] in {'.','!','?'}:\n",
    "            \n",
    "            # unambiguous punctuation that signifies end of a sentence\n",
    "            if tokens[index] in {'!','?'}:\n",
    "                result.append(sentence)\n",
    "                sentence = \"\"\n",
    "            \n",
    "            # ambiguous punctuation that may or may not signify end of a sentece\n",
    "            elif tokens[index] == '.':\n",
    "                # period [.] represents a decimal number\n",
    "                if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                    sentence = sentence + tokens[index] + tokens[index+1] \n",
    "                    skip = True\n",
    "               \n",
    "                # period [.] represents common abbreviations like e.g. and i.e.\n",
    "                elif index+1 < len(tokens) and tokens[index-1] in {'e', 'i'} and tokens[index+1] in {'g','e'}: \n",
    "                    sentence = sentence + tokens[index+1]\n",
    "                    skip = True\n",
    "                \n",
    "                # period[.] represents the end of an abbreviation with periods inbetween\n",
    "                elif sentence[-2:] in {'eg','ie'}: continue\n",
    "                    \n",
    "                # period [.] represents end of common abbreviations with no periods inbetween\n",
    "                elif index-1 >= 0 and tokens[index-1].lower() in {'am','pm','mr','ms','dr','mrs','inc','tbsp','tsp','gal','lb','lbs','qt','pt'}: continue\n",
    "                \n",
    "                # period [.] represents the end of a sentence\n",
    "                else: \n",
    "                    result.append(sentence)\n",
    "                    sentence = \"\"\n",
    "            \n",
    "            # punctuation represents token is a clause (, ; :)\n",
    "            # else: sentence = sentence + tokens[index]\n",
    "        \n",
    "        else: sentence = sentence + \" \" + tokens[index]\n",
    "    if sentence: result.append(sentence)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def segment_clause(sentences: List[str]):\n",
    "    result = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        clause = \"\"\n",
    "        tokens = tokenize_clauses(sentence)\n",
    "        \n",
    "        skip = False\n",
    "        for index in range(len(tokens)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            \n",
    "            # beginning of a new clause\n",
    "            elif clause == \"\": \n",
    "                clause = tokens[index]\n",
    "            \n",
    "            elif tokens[index] in {',',';',':'}:\n",
    "                # unambiguous punctuation that signifies end of a clause\n",
    "                if tokens[index] in {';'}:\n",
    "                    result.append(clause)\n",
    "                    clause = \"\"\n",
    "                \n",
    "                # ambiguous colons [:] that may or may not signify the end of a clause\n",
    "                elif tokens[index] == ':':\n",
    "                    # colon [:] represents a ratio\n",
    "                    if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                        clause = clause + tokens[index] + tokens[index+1] \n",
    "                        skip = True\n",
    "                    \n",
    "                    # colon [:] represents end of a clause\n",
    "                    else: \n",
    "                        result.append(clause)\n",
    "                        clause = \"\"\n",
    "                \n",
    "                # ambiguous commas [,] that may or may not signify the end of a clause\n",
    "                else: \n",
    "                    # comma [,] represents a number with commas\n",
    "                    if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                        clause = clause + tokens[index] + tokens[index+1] \n",
    "                        skip = True\n",
    "                        \n",
    "                    # comma [,] used to separate items in a list\n",
    "                    elif index-2 >= 0 and tokens[index-2] == ',' or index+2 < len(tokens) and tokens[index+2] == ',': continue\n",
    "                    \n",
    "                    # comma [,] represents end of a clause\n",
    "                    else: \n",
    "                        result.append(clause)\n",
    "                        clause = \"\"\n",
    "            \n",
    "            else: clause = clause + \" \" + tokens[index]\n",
    "\n",
    "        if clause:\n",
    "            result.append(clause)\n",
    "            clause = \"\"\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spell_check: I believe I had 1 avocado toast, 2 slices of tomatoes, and 3 pieces of chocolate. I'd rather have had an egg; they'd love that\n",
      "expand_contractions: I believe I had 1 avocado toast, 2 slices of tomatoes, and 3 pieces of chocolate. I would rather have had an egg; they would love that\n",
      "segment_sentence: ['I believe I had 1 avocado toast, 2 slices of tomatoes, and 3 pieces of chocolate', 'I would rather have had an egg; they would love that']\n",
      "segment_clause: ['I believe I had 1 avocado toast', '2 slices of tomatoes', 'and 3 pieces of chocolate', 'I would rather have had an egg', 'they would love that']\n"
     ]
    }
   ],
   "source": [
    "text = \"They'd can't beleeve you didn't do it!\"\n",
    "#print(tokenize_regex(text))\n",
    "# print(expand_contractions(text))\n",
    "\n",
    "\n",
    "#text = \"Hello there, my name is: Chloe Lam! I'd like a cheese, tomato, parmesan, and egg sandwich. I had 3.5 eggs, e.g. on toast at 5 pm. today. There has been 1:2 ratios; I would like 3,500 of those\"\n",
    "text = \"I beleeve I had 1 avocado toast, 2 slices of tomatoes, and 3 pieces of chocolate. I'd rather have had an egg; they'd love that\"\n",
    "text_sc = spell_check(text)\n",
    "text_ec = expand_contractions(text_sc)\n",
    "print(\"spell_check:\", text_sc)\n",
    "print(\"expand_contractions:\", text_ec)\n",
    "print(\"segment_sentence:\", segment_sentence(text_ec))\n",
    "print(\"segment_clause:\", segment_clause(segment_sentence(text_ec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert written words to numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}