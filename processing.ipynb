{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: word2number in ./venv/lib/python3.9/site-packages (1.1)\n",
      "Requirement already satisfied: num2words in ./venv/lib/python3.9/site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in ./venv/lib/python3.9/site-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: pyspellchecker in ./venv/lib/python3.9/site-packages (0.6.2)\n",
      "Requirement already satisfied: spacy in ./venv/lib/python3.9/site-packages (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.9/site-packages (from spacy) (4.60.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./venv/lib/python3.9/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in ./venv/lib/python3.9/site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./venv/lib/python3.9/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in ./venv/lib/python3.9/site-packages (from spacy) (8.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in ./venv/lib/python3.9/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in ./venv/lib/python3.9/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./venv/lib/python3.9/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in ./venv/lib/python3.9/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from spacy) (54.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.9/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in ./venv/lib/python3.9/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.9/site-packages (from spacy) (1.20.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.9/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in ./venv/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in ./venv/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./venv/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in ./venv/lib/python3.9/site-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.60.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in ./venv/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in ./venv/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./venv/lib/python3.9/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install word2number\n",
    "!pip install num2words\n",
    "!pip install pyspellchecker\n",
    "!pip install spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "from word2number import w2n\n",
    "from num2words import num2words\n",
    "from typing import List\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Processing\n",
    "# 1. Expand English contractions\n",
    "# 2. Convert written words to numbers\n",
    "# 3. Segment by sentences, then segment by clauses\n",
    "# 4. Spell checker\n",
    "# 5. Case folding, remove non-alphanumeric characters, remove stop-words\n",
    "# 6. Stemming and lemmatization\n",
    "# 7. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize methods\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # tokenize with . ! ? to segment sentences \n",
    "    RE_TOK = re.compile(r'([.!?]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_clauses(text):\n",
    "    # tokenize with , : ; to segment clauses \n",
    "    RE_TOK = re.compile(r'([,:;]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_regex(text):\n",
    "    # tokenize with . ! ? , : ; to segment sentences and clauses \n",
    "    RE_TOK = re.compile(r'([.,:]|\\s+)')\n",
    "\n",
    "    prev_idx = 0\n",
    "    tokens = []\n",
    "    for m in RE_TOK.finditer(text):\n",
    "        t = text[prev_idx:m.start()].strip()\n",
    "        if t: tokens.append(t)\n",
    "        t = m.group().strip()\n",
    "        if t: tokens.append(t)\n",
    "        prev_idx = m.end()\n",
    "\n",
    "    t = text[prev_idx:].strip()\n",
    "    if t: tokens.append(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand English Contractions \n",
    "# list of contractions from https://englishstudypage.com/grammar/list-of-contractions-in-english/\n",
    "\n",
    "# Read the English contractions list csv file and convert to dictionary where key = abbreviation, value = contracted words\n",
    "with open('english-contractions-list.csv', mode='r') as input:\n",
    "        reader = csv.reader(input)\n",
    "        contractionsDict = {rows[0]:rows[1] for rows in reader}\n",
    "        contractionsDict.pop('Abbreviation')\n",
    "    \n",
    "def expand_contractions(text):\n",
    "    result = text\n",
    "    tokens = tokenize_regex(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        # if the token is an abbreviation, then we will change it to its contracted English words\n",
    "        if contractionsDict.get(token.lower()) != None: \n",
    "            new = contractionsDict.get(token.lower())\n",
    "\n",
    "            # if the token was the beginning of a sentence, then we change the capitalization to match it\n",
    "            if token[:1].isupper(): new = new[:1].upper() + new[1:]   \n",
    "            result = result.replace(token, new)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert written words to numbers\n",
    "\n",
    "def word_to_number(text):\n",
    "    # regular expression to find written number words and digits\n",
    "    r = re.compile(r'(\\d)|\\b(zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|fourty|fifty|sixty|seventy|eight|ninety|hundred|thousand|million|billion|point)\\b')\n",
    "    r_digits = re.compile(r'(\\d)')\n",
    "    \n",
    "    result = text\n",
    "    tokens = tokenize_regex(text)\n",
    "    words = []\n",
    "    \n",
    "    fraction = False\n",
    "    mix = False\n",
    "    for index in range(len(tokens)):\n",
    "\n",
    "        # if we find a word that represents a number\n",
    "        if r.match(tokens[index].lower()): \n",
    "            words.append(tokens[index])\n",
    "        \n",
    "        # if we find a fraction\n",
    "        elif index+1 < len(tokens) and tokens[index].lower() == 'out' and tokens[index+1].lower() == 'of':\n",
    "            words.append(tokens[index])\n",
    "            words.append(tokens[index+1])\n",
    "            fraction = True\n",
    "            \n",
    "        # if we reach here, then tokens[index] is not a number word but our previous token(s) were number words\n",
    "        elif words:\n",
    "            old, replace = \"\", \"\"\n",
    "            for w in words: \n",
    "                # if there is a mix of digits and number words\n",
    "                if r_digits.match(w):\n",
    "                    replace = old + num2words(w) + \" \"\n",
    "                    mix = True \n",
    "                elif mix: replace = replace + w + \" \"\n",
    "                old = old + w + \" \"\n",
    "                    \n",
    "            old = old[:-1]\n",
    "            if replace: replace = replace[:-1]\n",
    "            \n",
    "            # if old contains \"out of\", then we add a slash for the fraction\n",
    "            if \"out of\" in old: \n",
    "                if r_digits.match(old): result = result.replace(\" out of\", \"/\")\n",
    "                else: result = result.replace(old, str(w2n.word_to_num(str(old))) + '/')\n",
    "                   \n",
    "            # if we didn't have this \"six out of ten\" would be converted to \"6/ 10\" instead of \"6/10\"\n",
    "            elif fraction: \n",
    "                result = result.replace(\" \" + old, str(w2n.word_to_num(old)))\n",
    "                fraction = False\n",
    "            \n",
    "            # if the text we are converting has a mix of words and digits (eg. \"fifty 4\")\n",
    "            elif mix: \n",
    "                result = result.replace(old, str(w2n.word_to_num(replace)))\n",
    "                mix = False\n",
    "                \n",
    "            # w2n.word_to_num() handles the conversion of the number words to number form\n",
    "            else: result = result.replace(old, str(w2n.word_to_num(old)))\n",
    "            words.clear()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence and Clause Segmentation\n",
    "\n",
    "r_digits = re.compile(r'\\d')\n",
    "\n",
    "def segment_sentence(text):\n",
    "    result = []\n",
    "    sentence = \"\"\n",
    "    tokens = tokenize_sentences(text)\n",
    "    \n",
    "    skip = False\n",
    "    for index in range(len(tokens)):   \n",
    "        \n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        \n",
    "        # beginning of a new sentence\n",
    "        elif sentence == \"\": sentence = tokens[index]\n",
    "            # sentence = tokens[index][:1].lower() + tokens[index][1:] #consider case folding\n",
    "        \n",
    "        # segment by sentences\n",
    "        elif tokens[index] in {'.','!','?'}:\n",
    "            \n",
    "            # unambiguous punctuation that signifies end of a sentence\n",
    "            if tokens[index] in {'!','?'}:\n",
    "                result.append(sentence)\n",
    "                sentence = \"\"\n",
    "            \n",
    "            # ambiguous punctuation that may or may not signify end of a sentece\n",
    "            elif tokens[index] == '.':\n",
    "                # period [.] represents a decimal number\n",
    "                if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                    sentence = sentence + tokens[index] + tokens[index+1] \n",
    "                    skip = True\n",
    "               \n",
    "                # period [.] represents common abbreviations like e.g. and i.e.\n",
    "                elif index+1 < len(tokens) and tokens[index-1] in {'e', 'i'} and tokens[index+1] in {'g','e'}: \n",
    "                    sentence = sentence + tokens[index+1]\n",
    "                    skip = True\n",
    "                \n",
    "                # period[.] represents the end of an abbreviation with periods inbetween\n",
    "                elif sentence[-2:] in {'eg','ie'}: continue\n",
    "                    \n",
    "                # period [.] represents end of common abbreviations with no periods inbetween\n",
    "                elif index-1 >= 0 and tokens[index-1].lower() in {'am','pm','mr','ms','dr','mrs','inc','tbsp','tsp','gal','lb','lbs','qt','pt'}: continue\n",
    "                \n",
    "                # period [.] represents the end of a sentence\n",
    "                else: \n",
    "                    result.append(sentence)\n",
    "                    sentence = \"\"\n",
    "            \n",
    "            # punctuation represents token is a clause (, ; :)\n",
    "            # else: sentence = sentence + tokens[index]\n",
    "        \n",
    "        else: sentence = sentence + \" \" + tokens[index]\n",
    "    if sentence: result.append(sentence)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def segment_clause(sentences: List[str]):\n",
    "    result = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        clause = \"\"\n",
    "        tokens = tokenize_clauses(sentence)\n",
    "        \n",
    "        skip = False\n",
    "        for index in range(len(tokens)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            \n",
    "            # beginning of a new clause\n",
    "            elif clause == \"\": \n",
    "                clause = tokens[index]\n",
    "            \n",
    "            elif tokens[index] in {',',';',':'}:\n",
    "                # unambiguous punctuation that signifies end of a clause\n",
    "                if tokens[index] in {';'}:\n",
    "                    result.append(clause)\n",
    "                    clause = \"\"\n",
    "                \n",
    "                # ambiguous colons [:] that may or may not signify the end of a clause\n",
    "                elif tokens[index] == ':':\n",
    "                    # colon [:] represents a ratio\n",
    "                    if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                        clause = clause + tokens[index] + tokens[index+1] \n",
    "                        skip = True\n",
    "                    \n",
    "                    # colon [:] represents end of a clause\n",
    "                    else: \n",
    "                        result.append(clause)\n",
    "                        clause = \"\"\n",
    "                \n",
    "                # ambiguous commas [,] that may or may not signify the end of a clause\n",
    "                else: \n",
    "                    # comma [,] represents a number with commas\n",
    "                    if r_digits.match(tokens[index-1]) and r_digits.match(tokens[index+1]): \n",
    "                        clause = clause + tokens[index] + tokens[index+1] \n",
    "                        skip = True\n",
    "                        \n",
    "                    # comma [,] used to separate items in a list\n",
    "                    elif index-2 >= 0 and tokens[index-2] == ',' or index+2 < len(tokens) and tokens[index+2] == ',': continue\n",
    "                    \n",
    "                    # comma [,] represents end of a clause\n",
    "                    else: \n",
    "                        result.append(clause)\n",
    "                        clause = \"\"\n",
    "            \n",
    "            else: clause = clause + \" \" + tokens[index]\n",
    "\n",
    "        if clause:\n",
    "            result.append(clause)\n",
    "            clause = \"\"\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Checker\n",
    "\n",
    "def spell_check(clauses: List[str]):\n",
    "    spell = SpellChecker()\n",
    "    result = clauses.copy()\n",
    "    \n",
    "    # for each clause in result\n",
    "    for index in range(len(result)):\n",
    "        \n",
    "        # split each clause by whitespaces\n",
    "        for word in result[index].split():\n",
    "            mispelled = spell.unknown(word)\n",
    "            if mispelled: result[index] = result[index].replace(word, spell.correction(word))\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "butter compound\n",
      "pancakes ROOT\n",
      "item:  ate\n",
      "token.is_stop:  False\n",
      "VERB\n",
      "ROOT\n",
      "\n",
      "item:  \n",
      "token.is_stop:  True\n",
      "DET\n",
      "ROOT\n",
      "\n",
      "item:  turkey\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  turkey cheese\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  turkey cheese sandwich\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  \n",
      "token.is_stop:  True\n",
      "ADP\n",
      "ROOT\n",
      "\n",
      "item:  apple\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  \n",
      "token.is_stop:  True\n",
      "CCONJ\n",
      "ROOT\n",
      "\n",
      "item:  glass\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  \n",
      "token.is_stop:  True\n",
      "ADP\n",
      "ROOT\n",
      "\n",
      "item:  water\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  butter\n",
      "token.is_stop:  False\n",
      "NOUN\n",
      "ROOT\n",
      "\n",
      "item:  butter pancakes\n",
      "token.is_stop:  False\n",
      "VERB\n",
      "ROOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ate', 'turkey cheese sandwich', 'apple', 'glass', 'water', 'butter pancakes']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mihir's added code:\n",
    "\n",
    "#  Case Folding\n",
    "def case_fold(clauses: List[str]):\n",
    "    for index in range(len(clauses)):\n",
    "        clauses[index] = clauses[index].lower()\n",
    "    return clauses\n",
    "\n",
    "# Removing non-alphanumeric characters\n",
    "def remove_non_alphanumeric(clauses: List[str]):\n",
    "    result = []\n",
    "    for clause in clauses:\n",
    "        clause = re.sub(r'[^a-zA-Z0-9./ ]', '', clause)\n",
    "        result.append(clause)\n",
    "    return result\n",
    "\n",
    "# # Removing stop-words\n",
    "# def remove_stop_words(clauses: List[str]):\n",
    "#     #\"Long Stopword List\" from https://www.ranks.nl/stopwords\n",
    "\n",
    "#     result = clauses.copy()\n",
    "#     stop_words_string = \"a able about above abst accordance according accordingly across act actually added adj affected affecting affects after afterwards again against ah all almost alone along already also although always am among amongst an and announce another any anybody anyhow anymore anyone anything anyway anyways anywhere apparently approximately are aren arent arise around as aside ask asking at auth available away awfully b back be became because become becomes becoming been before beforehand begin beginning beginnings begins behind being believe below beside besides between beyond biol both brief briefly but by c ca came can cannot can't cause causes certain certainly co com come comes contain containing contains could couldnt d date did didn't different do does doesn't doing done don't down downwards due during e each ed edu effect eg eight eighty either else elsewhere end ending enough especially et et-al etc even ever every everybody everyone everything everywhere ex except f far few ff fifth first five fix followed following follows for former formerly forth found four from further furthermore g gave get gets getting give given gives giving go goes gone got gotten h had happens hardly has hasn't have haven't having he hed hence her here hereafter hereby herein heres hereupon hers herself hes hi hid him himself his hither home how howbeit however hundred i id ie if i'll im immediate immediately importance important in inc indeed index information instead into invention inward is isn't it itd it'll its itself i've j just k keep\tkeeps kept kg km know known knows l largely last lately later latter latterly least less lest let lets like liked likely line little 'll look looking looks ltd m made mainly make makes many may maybe me mean means meantime meanwhile merely mg might million miss ml more moreover most mostly mr mrs much mug must my myself n na name namely nay nd near nearly necessarily necessary need needs neither never nevertheless new next nine ninety no nobody non none nonetheless noone nor normally nos not noted nothing now nowhere o obtain obtained obviously of off often oh ok okay old omitted on once one ones only onto or ord other others otherwise ought our ours ourselves out outside over overall owing own p page pages part particular particularly past per perhaps placed please plus poorly possible possibly potentially pp predominantly present previously primarily probably promptly proud provides put q que quickly quite qv r ran rather rd re readily really recent recently ref refs regarding regardless regards related relatively research respectively resulted resulting results right run s said same saw say saying says sec section see seeing seem seemed seeming seems seen self selves sent seven several shall she shed she'll shes should shouldn't show showed shown showns shows significant significantly similar similarly since six slightly so some somebody somehow someone somethan something sometime sometimes somewhat somewhere soon sorry specifically specified specify specifying still stop strongly sub substantially successfully such sufficiently suggest sup sure\tt take taken taking tell tends th than thank thanks thanx that that'll thats that've the their theirs them themselves then thence there thereafter thereby thered therefore therein there'll thereof therere theres thereto thereupon there've these they theyd they'll theyre they've think this those thou though thoughh thousand throug through throughout thru thus til tip to together too took toward towards tried tries truly try trying ts twice two u un under unfortunately unless unlike unlikely until unto up upon ups us use used useful usefully usefulness uses using usually v value various 've very via viz vol vols vs w want wants was wasnt way we wed welcome we'll went were werent we've what whatever what'll whats when whence whenever where whereafter whereas whereby wherein wheres whereupon wherever whether which while whim whither who whod whoever whole who'll whom whomever whos whose why widely willing wish with within without wont words world would wouldnt www x y yes yet you youd you'll your youre yours yourself yourselves you've z zero\"\n",
    "#     stop_words = stop_words_string.split()\n",
    "    \n",
    "#     for index in range(len(result)):\n",
    "#         filtered = []\n",
    "        \n",
    "#         for word in result[index].split():\n",
    "#             if word in stop_words: continue\n",
    "#             filtered.append(word)\n",
    "#         result[index] = \" \".join(filtered)\n",
    "#     return result\n",
    "\n",
    "# Removing stop-words\n",
    "def remove_stop_words(clauses: List[str]):\n",
    "    #\"Long Stopword List\" from https://www.ranks.nl/stopwords\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "#     result = clauses.copy()\n",
    "#     stop_words_string = \"able about above abst accordance according accordingly across act actually added adj affected affecting affects after afterwards again against ah all almost alone along already also although always am among amongst and announce another any anybody anyhow anymore anyone anything anyway anyways anywhere apparently approximately are aren arent arise around as aside ask asking at auth available away awfully b back be became because become becomes becoming been before beforehand begin beginning beginnings begins behind being believe below beside besides between beyond biol both brief briefly but by c ca came can cannot can't cause causes certain certainly co com come comes contain containing contains could couldnt d date did didn't different do does doesn't doing done don't down downwards due during e each ed edu effect eg eight eighty either else elsewhere end ending enough especially et et-al etc even ever every everybody everyone everything everywhere ex except f far few ff fifth first five fix followed following follows for former formerly forth found four from further furthermore g gave get gets getting give given gives giving go goes gone got gotten h had happens hardly has hasn't have haven't having he hed hence her here hereafter hereby herein heres hereupon hers herself hes hi hid him himself his hither home how howbeit however hundred i id ie if i'll im immediate immediately importance important in inc indeed index information instead into invention inward is isn't it itd it'll its itself i've j just k keep\tkeeps kept kg km know known knows l largely last lately later latter latterly least less lest let lets like liked likely line little 'll look looking looks ltd m made mainly make makes many may maybe me mean means meantime meanwhile merely mg might million miss ml more moreover most mostly mr mrs much mug must my myself n na name namely nay nd near nearly necessarily necessary need needs neither never nevertheless new next nine ninety no nobody non none nonetheless noone nor normally nos not noted nothing now nowhere o obtain obtained obviously off often oh ok okay old omitted on once one ones only onto or ord other others otherwise ought our ours ourselves out outside over overall owing own p page pages part particular particularly past per perhaps placed please plus poorly possible possibly potentially pp predominantly present previously primarily probably promptly proud provides put q que quickly quite qv r ran rather rd re readily really recent recently ref refs regarding regardless regards related relatively research respectively resulted resulting results right run s said same saw say saying says sec section see seeing seem seemed seeming seems seen self selves sent seven several shall she shed she'll shes should shouldn't show showed shown showns shows significant significantly similar similarly since six slightly so some somebody somehow someone somethan something sometime sometimes somewhat somewhere soon sorry specifically specified specify specifying still stop strongly sub substantially successfully such sufficiently suggest sup sure\tt take taken taking tell tends th than thank thanks thanx that that'll thats that've the their theirs them themselves then thence there thereafter thereby thered therefore therein there'll thereof therere theres thereto thereupon there've these they theyd they'll theyre they've think this those thou though thoughh thousand throug through throughout thru thus til tip to together too took toward towards tried tries truly try trying ts twice two u un under unfortunately unless unlike unlikely until unto up upon ups us use used useful usefully usefulness uses using usually v value various 've very via viz vol vols vs w want wants was wasnt way we wed welcome we'll went were werent we've what whatever what'll whats when whence whenever where whereafter whereas whereby wherein wheres whereupon wherever whether which while whim whither who whod whoever whole who'll whom whomever whos whose why widely willing wish with within without wont words world would wouldnt www x y yes yet you youd you'll your youre yours yourself yourselves you've z zero\"\n",
    "#     stop_words = stop_words_string.split()\n",
    "    \n",
    "    item = \"\"\n",
    "    filtered = []\n",
    "    for index in range(len(clauses)):\n",
    "         \n",
    "        for word in clauses[index].split():\n",
    "            doc = nlp(word)\n",
    "#                 print(\"word: \", word)\n",
    "            for token in doc:\n",
    "#                 print(token.text, token.pos_, token.is_stop)\n",
    "#                 print(\"verb? \", token.pos_ == 'VERB')\n",
    "            \n",
    "#                 if (token.is_stop or token.pos_ == 'VERB') and item == \"\": continue\n",
    "#                 elif (token.is_stop or token.pos_ == 'VERB') and item != \"\":\n",
    "#                     filtered.append(item)\n",
    "#                     item = \"\"\n",
    "#                 elif token.is_stop == False and item == \"\": item = word\n",
    "#                 elif token.is_stop == False and item != \"\": item = item + \" \" + word\n",
    "                if (token.is_stop) and item == \"\": continue\n",
    "                elif (token.is_stop) and item != \"\":\n",
    "                    filtered.append(item)\n",
    "                    item = \"\"\n",
    "                elif token.is_stop == False and item == \"\": item = word\n",
    "                elif token.is_stop == False and item != \"\": item = item + \" \" + word\n",
    "\n",
    "                print(\"item: \", item)\n",
    "                print(\"token.is_stop: \", token.is_stop)\n",
    "                print(token.pos_)\n",
    "                print(token.dep_)\n",
    "                print()\n",
    "        if item != \"\": \n",
    "            filtered.append(item)\n",
    "            item = \"\"\n",
    "\n",
    "#     if item != \"\": filtered.append(item)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "doc = nlp('butter pancakes')\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)\n",
    "\n",
    "# remove_stop_words(['i ate a turkey cheese sandwich with an apple and a glass of water'])\n",
    "remove_stop_words(['i ate a turkey cheese sandwich with an apple and a glass of water', 'and butter pancakes'])\n",
    "# remove_stop_words(['i believe i had 1 avocado toast', '2.5 slices of tomatoes', 'and 3/4 pieces of chocolate', 'i would rather have had an egg', 'they would love that'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat turkey cheese sandwich apple glass water']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming and lemmatization\n",
    "def lemmatize(clauses:List[str]):\n",
    "    spacy_use = spacy.load(\"en_core_web_sm\")\n",
    "    lemmatizer = spacy_use.get_pipe(\"lemmatizer\")\n",
    "\n",
    "    for clause in clauses:\n",
    "        doc = spacy_use(clause)\n",
    "        new_clause = \"\"\n",
    "        for token in doc:\n",
    "            if token is not token.lemma_:\n",
    "                if new_clause == \"\": new_clause = token.lemma_\n",
    "                else: new_clause = new_clause + \" \" + token.lemma_\n",
    "            else:\n",
    "                if new_clause == \"\": new_clause = token\n",
    "                else: new_clause = new_clause + \" \" + token.lemma_\n",
    "        clauses[clauses.index(clause)] = new_clause\n",
    "        \n",
    "    return clauses\n",
    "        \n",
    "lemmatize(['ate turkey cheese sandwich apple glass water'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: I beleeve I had one avocado toast, 2 point five slices of tomatoes, and 3 out of 4 pieces of chocolate. I'd rather have had an egg; they'd love that\n",
      "\n",
      "expand_contractions: I beleeve I had one avocado toast, 2 point five slices of tomatoes, and 3 out of 4 pieces of chocolate. I would rather have had an egg; they would love that\n",
      "\n",
      "word to numbers: I beleeve I had 1 avocado toast, 2.5 slices of tomatoes, and 3/4 pieces of chocolate. I would rather have had an egg; they would love that\n",
      "\n",
      "segment_sentence: ['I beleeve I had 1 avocado toast, 2.5 slices of tomatoes, and 3/4 pieces of chocolate', 'I would rather have had an egg; they would love that']\n",
      "\n",
      "segment_clause: ['I beleeve I had 1 avocado toast', '2.5 slices of tomatoes', 'and 3/4 pieces of chocolate', 'I would rather have had an egg', 'they would love that']\n",
      "\n",
      "spell_check: ['i believe i had 1 avocado toast', '2.5 slices of tomatoes', 'and 3/4 pieces of chocolate', 'i would rather have had an egg', 'they would love that']\n",
      "\n",
      "case_fold: ['i believe i had 1 avocado toast', '2.5 slices of tomatoes', 'and 3/4 pieces of chocolate', 'i would rather have had an egg', 'they would love that']\n",
      "\n",
      "remove_non_alphanumeric: ['i believe i had 1 avocado toast', '2.5 slices of tomatoes', 'and 3/4 pieces of chocolate', 'i would rather have had an egg', 'they would love that']\n",
      "\n",
      "remove_stop_words: ['egg', 'love']\n"
     ]
    }
   ],
   "source": [
    "#text = \"Hello there, my name is: Chloe Lam! I'd like a cheese, tomato, parmesan, and egg sandwich. I had 3.5 eggs, e.g. on toast at 5 pm. today. There has been 1:2 ratios; I would like 3,500 of those\"\n",
    "text = \"I beleeve I had one avocado toast, 2 point five slices of tomatoes, and 3 out of 4 pieces of chocolate. I'd rather have had an egg; they'd love that\"\n",
    "# text = \"I had 1 cup of basmati rice with chicken tikka masala\"\n",
    "# text = \"I ate a turkey cheese sandwich with an apple and a glass of water\"\n",
    "\n",
    "text_ec = expand_contractions(text)\n",
    "text_w2n = word_to_number(text_ec)\n",
    "text_ss = segment_sentence(text_w2n)\n",
    "text_sc = segment_clause(text_ss)\n",
    "text_sp = spell_check(text_sc)\n",
    "text_cf = case_fold(text_sp)\n",
    "text_rna = remove_non_alphanumeric(text_cf)\n",
    "text_rsw = remove_stop_words(text_rna)\n",
    "\n",
    "print(\"original:\", text)\n",
    "print()\n",
    "print(\"expand_contractions:\", text_ec)\n",
    "print()\n",
    "print(\"word to numbers:\", text_w2n)\n",
    "print()\n",
    "print(\"segment_sentence:\", text_ss)\n",
    "print()\n",
    "print(\"segment_clause:\", text_sc)\n",
    "print()\n",
    "print(\"spell_check:\", text_sp)\n",
    "print()\n",
    "print(\"case_fold:\", text_cf)\n",
    "print()\n",
    "print(\"remove_non_alphanumeric:\", text_rna)\n",
    "print()\n",
    "print(\"remove_stop_words:\", text_rsw)\n",
    "\n",
    "# Processing\n",
    "# 1. Expand English contractions\n",
    "# 2. Convert written words to numbers\n",
    "# 3. Segment by sentences, then segment by clauses\n",
    "# 4. Spell checker\n",
    "# 5. Case folding, remove non-alphanumeric characters, remove stop-words\n",
    "# 6. Stemming and lemmatization\n",
    "# 7. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "a turkey cheese sandwich\n",
      "an apple\n",
      "a glass\n",
      "water and butter pancakes\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"ate turkey cheese sandwich apple tablespoon cup water McDonald's 1 one\")\n",
    "    \n",
    "# print(doc.ents)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Apple is the first U.S. public company to reach a $1 trillion market value\")\n",
    "doc = nlp(\"I ate a turkey cheese sandwich with an apple and a glass of water and butter pancakes\")\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_, token.head.text, token.head.pos_,\n",
    "#             [child for child in token.children])\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)\n",
    "\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
